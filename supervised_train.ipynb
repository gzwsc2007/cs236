{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82dee64-b4b6-4415-9af8-c10d6ad95601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mansonw\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "import utils\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_paths =  [\n",
    "    os.path.abspath(os.path.join('ronin/source'))  # RoNIN\n",
    "]\n",
    "for module_path in module_paths:\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "import data_glob_speed\n",
    "import data_ridi\n",
    "import cnn_vae_model\n",
    "\n",
    "# WANDB API Key: eefeec3d5632912a6bb9112f48d2dde3ca6e0658\n",
    "wandb.login()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcab873c-af72-4da7-844e-f3b82abdf135",
   "metadata": {},
   "source": [
    "# Load RONIN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5089478-72a4-455f-8377-f00daf915815",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR = 'datasets'\n",
    "with open('datasets/self_sup_ronin_train_list.txt') as f:\n",
    "    ronin_data_list = [s.strip().split(',' or ' ')[0] for s in f.readlines() if len(s) > 0 and s[0] != '#']\n",
    "\n",
    "# Each item in the dataset is a (feature, target, seq_id, frame_id) tuple.\n",
    "# Each feature is a 6x200 array. Rows 0-2 are gyro, and rows 3-5 are accel (non gravity subtracted).\n",
    "# Both gyro and accels are in a gravity-aligned world frame (arbitrary yaw, but consistent throughout\n",
    "# the 200 frames)\n",
    "ronin_train_dataset = data_glob_speed.StridedSequenceDataset(data_glob_speed.GlobSpeedSequence,\n",
    "                                                             DATA_ROOT_DIR,\n",
    "                                                             ronin_data_list,\n",
    "                                                             cache_path='datasets/cache')\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(ronin_train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a946198d-1b59-4daa-97dd-8da2f0a3802f",
   "metadata": {},
   "source": [
    "# Load pre-trained CNN encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b2ac2-068b-4e9c-a692-74e15512d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the pre-trained CnnVae. Must match the saved model.\n",
    "\n",
    "# params from first try\n",
    "#latent_dim = 64\n",
    "#first_chan_size = 64\n",
    "#last_chan_size = 512\n",
    "#vae_fc_dim = 256\n",
    "\n",
    "latent_dim = 128\n",
    "first_chan_size = 32\n",
    "last_chan_size = 128\n",
    "vae_fc_dim = 128\n",
    "vae_model = cnn_vae_model.CnnVae(feature_dim=6,\n",
    "                                 latent_dim=latent_dim,\n",
    "                                 first_channel_size=first_chan_size,\n",
    "                                 last_channel_size=last_chan_size,\n",
    "                                 fc_dim=vae_fc_dim).to(device)\n",
    "\n",
    "# Load model from checkpoint.\n",
    "VAE_EPOCH_TO_LOAD = 496\n",
    "utils.load_model_by_name(vae_model, epoch=VAE_EPOCH_TO_LOAD)\n",
    "\n",
    "# Freeze encoder weights\n",
    "freeze_encoder = True\n",
    "if freeze_encoder:\n",
    "    print(\"Freezing encoder\")\n",
    "    for param in vae_model.enc.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae5566f-20e6-43b3-b116-82fdc13aa8bc",
   "metadata": {},
   "source": [
    "# Supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbab8ec0-2ea7-4fcd-aa56-ef826433f049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:i70syaoc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>vel_x_loss</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>vel_y_loss</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>0.0139</td></tr><tr><td>vel_x_loss</td><td>0.013</td></tr><tr><td>vel_y_loss</td><td>0.01479</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glorious-mountain-1</strong> at: <a href='https://wandb.ai/ansonw/Pretrained-Encoder-With-Velocity-Decoder-RONIN-dataset-supervised-unfreeze-encoder/runs/i70syaoc' target=\"_blank\">https://wandb.ai/ansonw/Pretrained-Encoder-With-Velocity-Decoder-RONIN-dataset-supervised-unfreeze-encoder/runs/i70syaoc</a><br/> View job at <a href='https://wandb.ai/ansonw/Pretrained-Encoder-With-Velocity-Decoder-RONIN-dataset-supervised-unfreeze-encoder/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMDQxMDU0OQ==/version_details/v0' target=\"_blank\">https://wandb.ai/ansonw/Pretrained-Encoder-With-Velocity-Decoder-RONIN-dataset-supervised-unfreeze-encoder/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMDQxMDU0OQ==/version_details/v0</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231202_103720-i70syaoc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:i70syaoc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e1f2bf693d4568bd13304a505a7446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111965588901917, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anson/cs236/project/wandb/run-20231202_170215-mk4dm5to</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ansonw/Pretrained-Encoder-With-Velocity-Decoder-RONIN-dataset-supervised/runs/mk4dm5to' target=\"_blank\">brisk-disco-3</a></strong> to <a href='https://wandb.ai/ansonw/Pretrained-Encoder-With-Velocity-Decoder-RONIN-dataset-supervised' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ansonw/Pretrained-Encoder-With-Velocity-Decoder-RONIN-dataset-supervised' target=\"_blank\">https://wandb.ai/ansonw/Pretrained-Encoder-With-Velocity-Decoder-RONIN-dataset-supervised</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ansonw/Pretrained-Encoder-With-Velocity-Decoder-RONIN-dataset-supervised/runs/mk4dm5to' target=\"_blank\">https://wandb.ai/ansonw/Pretrained-Encoder-With-Velocity-Decoder-RONIN-dataset-supervised/runs/mk4dm5to</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Epoch 0, time usage: 37.431s, average loss: [0.26295304 0.3417037 ]/0.302328\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00000.pt\n",
      "-------------------------\n",
      "Epoch 1, time usage: 38.599s, average loss: [0.23847719 0.30437276]/0.271425\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00001.pt\n",
      "-------------------------\n",
      "Epoch 2, time usage: 39.234s, average loss: [0.2314703  0.29271916]/0.262095\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00002.pt\n",
      "-------------------------\n",
      "Epoch 3, time usage: 39.265s, average loss: [0.22686137 0.2848245 ]/0.255843\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00003.pt\n",
      "-------------------------\n",
      "Epoch 4, time usage: 39.483s, average loss: [0.22370884 0.28079703]/0.252253\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00004.pt\n",
      "-------------------------\n",
      "Epoch 5, time usage: 39.456s, average loss: [0.22109675 0.27784   ]/0.249468\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00005.pt\n",
      "-------------------------\n",
      "Epoch 6, time usage: 40.341s, average loss: [0.21945697 0.27561256]/0.247535\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00006.pt\n",
      "-------------------------\n",
      "Epoch 7, time usage: 39.624s, average loss: [0.21800551 0.2736434 ]/0.245824\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00007.pt\n",
      "-------------------------\n",
      "Epoch 8, time usage: 38.895s, average loss: [0.21700819 0.2724515 ]/0.244730\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00008.pt\n",
      "-------------------------\n",
      "Epoch 9, time usage: 39.112s, average loss: [0.21624479 0.27135834]/0.243802\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00009.pt\n",
      "-------------------------\n",
      "Epoch 10, time usage: 39.403s, average loss: [0.21526769 0.27043873]/0.242853\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00010.pt\n",
      "-------------------------\n",
      "Epoch 11, time usage: 39.060s, average loss: [0.21460843 0.270114  ]/0.242361\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00011.pt\n",
      "-------------------------\n",
      "Epoch 12, time usage: 39.080s, average loss: [0.21442209 0.2694625 ]/0.241942\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00012.pt\n",
      "-------------------------\n",
      "Epoch 13, time usage: 39.700s, average loss: [0.21398549 0.26882392]/0.241405\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00013.pt\n",
      "-------------------------\n",
      "Epoch 14, time usage: 39.816s, average loss: [0.21352905 0.26851842]/0.241024\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00014.pt\n",
      "-------------------------\n",
      "Epoch 15, time usage: 40.203s, average loss: [0.2130606 0.2681144]/0.240588\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00015.pt\n",
      "-------------------------\n",
      "Epoch 16, time usage: 39.322s, average loss: [0.2131965  0.26757488]/0.240386\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00016.pt\n",
      "-------------------------\n",
      "Epoch 17, time usage: 38.429s, average loss: [0.21250504 0.26718011]/0.239843\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00017.pt\n",
      "-------------------------\n",
      "Epoch 18, time usage: 38.395s, average loss: [0.21213819 0.26714015]/0.239639\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00018.pt\n",
      "-------------------------\n",
      "Epoch 19, time usage: 38.585s, average loss: [0.21217233 0.26639366]/0.239283\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00019.pt\n",
      "-------------------------\n",
      "Epoch 20, time usage: 38.899s, average loss: [0.21206184 0.2662728 ]/0.239167\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00020.pt\n",
      "-------------------------\n",
      "Epoch 21, time usage: 39.189s, average loss: [0.21170627 0.26609322]/0.238900\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00021.pt\n",
      "-------------------------\n",
      "Epoch 22, time usage: 38.654s, average loss: [0.21173853 0.26616642]/0.238952\n",
      "-------------------------\n",
      "Epoch 23, time usage: 39.345s, average loss: [0.21158376 0.26606363]/0.238824\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00023.pt\n",
      "-------------------------\n",
      "Epoch 24, time usage: 39.070s, average loss: [0.21173902 0.2655084 ]/0.238624\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00024.pt\n",
      "-------------------------\n",
      "Epoch 25, time usage: 38.851s, average loss: [0.21174242 0.2658573 ]/0.238800\n",
      "-------------------------\n",
      "Epoch 26, time usage: 39.082s, average loss: [0.21158327 0.26564065]/0.238612\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00026.pt\n",
      "-------------------------\n",
      "Epoch 27, time usage: 39.465s, average loss: [0.21149059 0.26552138]/0.238506\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00027.pt\n",
      "-------------------------\n",
      "Epoch 28, time usage: 39.395s, average loss: [0.21139646 0.2655807 ]/0.238489\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00028.pt\n",
      "-------------------------\n",
      "Epoch 29, time usage: 38.904s, average loss: [0.21159765 0.26542035]/0.238509\n",
      "-------------------------\n",
      "Epoch 30, time usage: 38.786s, average loss: [0.21122023 0.26519412]/0.238207\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00030.pt\n",
      "-------------------------\n",
      "Epoch 31, time usage: 38.976s, average loss: [0.21146096 0.26552317]/0.238492\n",
      "-------------------------\n",
      "Epoch 32, time usage: 38.795s, average loss: [0.21115704 0.26517764]/0.238167\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00032.pt\n",
      "-------------------------\n",
      "Epoch 33, time usage: 39.180s, average loss: [0.21123813 0.2652719 ]/0.238255\n",
      "-------------------------\n",
      "Epoch 34, time usage: 39.046s, average loss: [0.2113949  0.26500398]/0.238199\n",
      "-------------------------\n",
      "Epoch 35, time usage: 38.855s, average loss: [0.21138965 0.26516646]/0.238278\n",
      "-------------------------\n",
      "Epoch 36, time usage: 39.289s, average loss: [0.211143  0.2648117]/0.237977\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00036.pt\n",
      "-------------------------\n",
      "Epoch 37, time usage: 39.553s, average loss: [0.21071747 0.26516297]/0.237940\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00037.pt\n",
      "-------------------------\n",
      "Epoch 38, time usage: 39.657s, average loss: [0.21077198 0.26465276]/0.237712\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00038.pt\n",
      "-------------------------\n",
      "Epoch 39, time usage: 38.999s, average loss: [0.21084714 0.26492375]/0.237885\n",
      "-------------------------\n",
      "Epoch 40, time usage: 39.856s, average loss: [0.2107462  0.26449668]/0.237621\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00040.pt\n",
      "-------------------------\n",
      "Epoch 41, time usage: 39.313s, average loss: [0.21069686 0.26505733]/0.237877\n",
      "-------------------------\n",
      "Epoch 42, time usage: 39.389s, average loss: [0.21090247 0.26462877]/0.237766\n",
      "-------------------------\n",
      "Epoch 43, time usage: 38.944s, average loss: [0.21091542 0.26454273]/0.237729\n",
      "-------------------------\n",
      "Epoch 44, time usage: 39.774s, average loss: [0.21061891 0.26450914]/0.237564\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00044.pt\n",
      "-------------------------\n",
      "Epoch 45, time usage: 38.877s, average loss: [0.21076486 0.26484057]/0.237803\n",
      "-------------------------\n",
      "Epoch 46, time usage: 39.766s, average loss: [0.21110252 0.26476488]/0.237934\n",
      "-------------------------\n",
      "Epoch 47, time usage: 39.386s, average loss: [0.2107523  0.26509547]/0.237924\n",
      "-------------------------\n",
      "Epoch 48, time usage: 38.737s, average loss: [0.21058631 0.2644415 ]/0.237514\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00048.pt\n",
      "-------------------------\n",
      "Epoch 49, time usage: 39.018s, average loss: [0.21053109 0.26506317]/0.237797\n",
      "-------------------------\n",
      "Epoch 50, time usage: 38.924s, average loss: [0.21084009 0.2643538 ]/0.237597\n",
      "-------------------------\n",
      "Epoch 51, time usage: 38.577s, average loss: [0.21076019 0.26456878]/0.237664\n",
      "-------------------------\n",
      "Epoch 52, time usage: 39.067s, average loss: [0.21082768 0.26406673]/0.237447\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00052.pt\n",
      "-------------------------\n",
      "Epoch 53, time usage: 38.851s, average loss: [0.21064596 0.2648615 ]/0.237754\n",
      "-------------------------\n",
      "Epoch 54, time usage: 38.701s, average loss: [0.21068668 0.26432818]/0.237507\n",
      "-------------------------\n",
      "Epoch 55, time usage: 38.731s, average loss: [0.21048018 0.26447958]/0.237480\n",
      "-------------------------\n",
      "Epoch 56, time usage: 39.066s, average loss: [0.21064961 0.26472816]/0.237689\n",
      "-------------------------\n",
      "Epoch 57, time usage: 38.968s, average loss: [0.2103796 0.2649379]/0.237659\n",
      "-------------------------\n",
      "Epoch 58, time usage: 38.212s, average loss: [0.2102628  0.26445803]/0.237360\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00058.pt\n",
      "-------------------------\n",
      "Epoch 59, time usage: 36.306s, average loss: [0.21050507 0.26443797]/0.237472\n",
      "-------------------------\n",
      "Epoch 60, time usage: 36.420s, average loss: [0.21073912 0.2643461 ]/0.237543\n",
      "-------------------------\n",
      "Epoch 61, time usage: 36.404s, average loss: [0.21052471 0.2646518 ]/0.237588\n",
      "-------------------------\n",
      "Epoch 62, time usage: 36.706s, average loss: [0.21034075 0.26407063]/0.237206\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00062.pt\n",
      "-------------------------\n",
      "Epoch 63, time usage: 36.523s, average loss: [0.21057723 0.2645218 ]/0.237550\n",
      "-------------------------\n",
      "Epoch 64, time usage: 36.651s, average loss: [0.21052487 0.26415235]/0.237339\n",
      "-------------------------\n",
      "Epoch 65, time usage: 36.178s, average loss: [0.21053839 0.26435673]/0.237448\n",
      "-------------------------\n",
      "Epoch 66, time usage: 36.402s, average loss: [0.21068405 0.2641746 ]/0.237429\n",
      "-------------------------\n",
      "Epoch 67, time usage: 36.402s, average loss: [0.21011263 0.26395905]/0.237036\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00067.pt\n",
      "-------------------------\n",
      "Epoch 68, time usage: 36.504s, average loss: [0.21039715 0.26440492]/0.237401\n",
      "-------------------------\n",
      "Epoch 69, time usage: 36.567s, average loss: [0.21051747 0.26432195]/0.237420\n",
      "-------------------------\n",
      "Epoch 70, time usage: 36.518s, average loss: [0.21048062 0.26391461]/0.237198\n",
      "-------------------------\n",
      "Epoch 71, time usage: 36.547s, average loss: [0.21048354 0.26407593]/0.237280\n",
      "-------------------------\n",
      "Epoch 72, time usage: 36.676s, average loss: [0.21025872 0.26388577]/0.237072\n",
      "-------------------------\n",
      "Epoch 73, time usage: 36.646s, average loss: [0.21029541 0.26449507]/0.237395\n",
      "-------------------------\n",
      "Epoch 74, time usage: 36.543s, average loss: [0.21029519 0.26452568]/0.237410\n",
      "-------------------------\n",
      "Epoch 75, time usage: 36.667s, average loss: [0.2106601  0.26421806]/0.237439\n",
      "-------------------------\n",
      "Epoch 76, time usage: 36.167s, average loss: [0.21074286 0.26370957]/0.237226\n",
      "-------------------------\n",
      "Epoch 77, time usage: 36.633s, average loss: [0.21045427 0.26481047]/0.237632\n",
      "-------------------------\n",
      "Epoch 78, time usage: 36.473s, average loss: [0.21076111 0.26381618]/0.237289\n",
      "-------------------------\n",
      "Epoch 79, time usage: 36.402s, average loss: [0.21060815 0.2643451 ]/0.237477\n",
      "-------------------------\n",
      "Epoch 80, time usage: 36.346s, average loss: [0.21079949 0.26424485]/0.237522\n",
      "-------------------------\n",
      "Epoch 81, time usage: 36.423s, average loss: [0.21061443 0.2644987 ]/0.237557\n",
      "-------------------------\n",
      "Epoch 82, time usage: 37.063s, average loss: [0.21024394 0.2638331 ]/0.237039\n",
      "-------------------------\n",
      "Epoch 83, time usage: 36.623s, average loss: [0.21035184 0.26370004]/0.237026\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00083.pt\n",
      "-------------------------\n",
      "Epoch 84, time usage: 36.588s, average loss: [0.21039811 0.26361817]/0.237008\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00084.pt\n",
      "-------------------------\n",
      "Epoch 85, time usage: 36.475s, average loss: [0.21031901 0.26406324]/0.237191\n",
      "-------------------------\n",
      "Epoch 86, time usage: 36.337s, average loss: [0.21032773 0.26375026]/0.237039\n",
      "-------------------------\n",
      "Epoch 87, time usage: 36.319s, average loss: [0.21004853 0.2642407 ]/0.237145\n",
      "-------------------------\n",
      "Epoch 88, time usage: 36.383s, average loss: [0.2103658 0.2641089]/0.237237\n",
      "-------------------------\n",
      "Epoch 89, time usage: 36.774s, average loss: [0.21061862 0.26420277]/0.237411\n",
      "-------------------------\n",
      "Epoch 90, time usage: 36.478s, average loss: [0.21057303 0.2637025 ]/0.237138\n",
      "-------------------------\n",
      "Epoch 91, time usage: 36.310s, average loss: [0.2101634  0.26418963]/0.237177\n",
      "-------------------------\n",
      "Epoch 92, time usage: 36.669s, average loss: [0.21027705 0.2638571 ]/0.237067\n",
      "-------------------------\n",
      "Epoch 93, time usage: 36.641s, average loss: [0.21003115 0.26400584]/0.237018\n",
      "-------------------------\n",
      "Epoch 94, time usage: 36.668s, average loss: [0.21038282 0.2639062 ]/0.237145\n",
      "-------------------------\n",
      "Epoch 95, time usage: 36.361s, average loss: [0.21038412 0.26415178]/0.237268\n",
      "-------------------------\n",
      "Epoch 96, time usage: 36.416s, average loss: [0.21024454 0.26403472]/0.237140\n",
      "-------------------------\n",
      "Epoch 97, time usage: 36.620s, average loss: [0.21042664 0.26334128]/0.236884\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00097.pt\n",
      "-------------------------\n",
      "Epoch 98, time usage: 36.495s, average loss: [0.21036352 0.26402733]/0.237195\n",
      "-------------------------\n",
      "Epoch 99, time usage: 36.495s, average loss: [0.21044347 0.26368463]/0.237064\n",
      "-------------------------\n",
      "Epoch 100, time usage: 36.428s, average loss: [0.2103643  0.26406363]/0.237214\n",
      "-------------------------\n",
      "Epoch 101, time usage: 36.426s, average loss: [0.20997001 0.26396725]/0.236969\n",
      "-------------------------\n",
      "Epoch 102, time usage: 36.414s, average loss: [0.21052018 0.2638672 ]/0.237194\n",
      "-------------------------\n",
      "Epoch 103, time usage: 36.499s, average loss: [0.21009316 0.26419285]/0.237143\n",
      "-------------------------\n",
      "Epoch 104, time usage: 36.931s, average loss: [0.21031524 0.2638049 ]/0.237060\n",
      "-------------------------\n",
      "Epoch 105, time usage: 36.359s, average loss: [0.21035217 0.26350096]/0.236927\n",
      "-------------------------\n",
      "Epoch 106, time usage: 36.211s, average loss: [0.21052787 0.264025  ]/0.237276\n",
      "-------------------------\n",
      "Epoch 107, time usage: 36.613s, average loss: [0.21027087 0.26381424]/0.237043\n",
      "-------------------------\n",
      "Epoch 108, time usage: 36.635s, average loss: [0.21032806 0.2641423 ]/0.237235\n",
      "-------------------------\n",
      "Epoch 109, time usage: 36.449s, average loss: [0.21015167 0.2638533 ]/0.237002\n",
      "-------------------------\n",
      "Epoch 110, time usage: 36.386s, average loss: [0.21030986 0.2641018 ]/0.237206\n",
      "-------------------------\n",
      "Epoch 111, time usage: 36.481s, average loss: [0.21009186 0.26400506]/0.237048\n",
      "-------------------------\n",
      "Epoch 112, time usage: 36.260s, average loss: [0.21056725 0.26439345]/0.237480\n",
      "-------------------------\n",
      "Epoch 113, time usage: 36.583s, average loss: [0.21049319 0.2640355 ]/0.237264\n",
      "-------------------------\n",
      "Epoch 114, time usage: 36.635s, average loss: [0.21020967 0.26423982]/0.237225\n",
      "-------------------------\n",
      "Epoch 115, time usage: 36.523s, average loss: [0.21007317 0.26385328]/0.236963\n",
      "-------------------------\n",
      "Epoch 116, time usage: 36.700s, average loss: [0.21028082 0.26396075]/0.237121\n",
      "-------------------------\n",
      "Epoch 117, time usage: 36.659s, average loss: [0.21037331 0.26383978]/0.237107\n",
      "-------------------------\n",
      "Epoch 118, time usage: 36.638s, average loss: [0.21042778 0.2637746 ]/0.237101\n",
      "-------------------------\n",
      "Epoch 119, time usage: 36.563s, average loss: [0.21006538 0.26375723]/0.236911\n",
      "-------------------------\n",
      "Epoch 120, time usage: 36.389s, average loss: [0.21001056 0.26369578]/0.236853\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00120.pt\n",
      "-------------------------\n",
      "Epoch 121, time usage: 37.175s, average loss: [0.21016833 0.26356572]/0.236867\n",
      "-------------------------\n",
      "Epoch 122, time usage: 36.373s, average loss: [0.21046472 0.26331732]/0.236891\n",
      "-------------------------\n",
      "Epoch 123, time usage: 36.438s, average loss: [0.21004172 0.2641285 ]/0.237085\n",
      "-------------------------\n",
      "Epoch 124, time usage: 36.569s, average loss: [0.21050788 0.26376942]/0.237139\n",
      "-------------------------\n",
      "Epoch 125, time usage: 36.550s, average loss: [0.21027568 0.26385647]/0.237066\n",
      "-------------------------\n",
      "Epoch 126, time usage: 36.688s, average loss: [0.21044013 0.26349878]/0.236969\n",
      "-------------------------\n",
      "Epoch 127, time usage: 35.980s, average loss: [0.21026042 0.2641713 ]/0.237216\n",
      "-------------------------\n",
      "Epoch 128, time usage: 36.410s, average loss: [0.21011461 0.26393902]/0.237027\n",
      "-------------------------\n",
      "Epoch 129, time usage: 36.969s, average loss: [0.21014088 0.26346272]/0.236802\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00129.pt\n",
      "-------------------------\n",
      "Epoch 130, time usage: 36.664s, average loss: [0.2102956  0.26387346]/0.237085\n",
      "-------------------------\n",
      "Epoch 131, time usage: 36.506s, average loss: [0.21030065 0.26423308]/0.237267\n",
      "-------------------------\n",
      "Epoch 132, time usage: 36.360s, average loss: [0.21030036 0.2638871 ]/0.237094\n",
      "-------------------------\n",
      "Epoch 133, time usage: 36.741s, average loss: [0.21044014 0.2637784 ]/0.237109\n",
      "-------------------------\n",
      "Epoch 134, time usage: 36.486s, average loss: [0.21006884 0.26447895]/0.237274\n",
      "-------------------------\n",
      "Epoch 135, time usage: 36.410s, average loss: [0.21034884 0.2636182 ]/0.236984\n",
      "-------------------------\n",
      "Epoch 136, time usage: 36.567s, average loss: [0.20972939 0.26404846]/0.236889\n",
      "-------------------------\n",
      "Epoch 137, time usage: 36.316s, average loss: [0.21034741 0.26381797]/0.237083\n",
      "-------------------------\n",
      "Epoch 138, time usage: 36.383s, average loss: [0.21015216 0.26367658]/0.236914\n",
      "-------------------------\n",
      "Epoch 139, time usage: 36.409s, average loss: [0.21011257 0.26370776]/0.236910\n",
      "-------------------------\n",
      "Epoch 140, time usage: 36.230s, average loss: [0.21022956 0.26382494]/0.237027\n",
      "-------------------------\n",
      "Epoch 141, time usage: 36.375s, average loss: [0.21064836 0.26361695]/0.237133\n",
      "-------------------------\n",
      "Epoch 142, time usage: 36.356s, average loss: [0.21042506 0.2634431 ]/0.236934\n",
      "-------------------------\n",
      "Epoch 143, time usage: 36.328s, average loss: [0.21007659 0.26395842]/0.237018\n",
      "-------------------------\n",
      "Epoch 144, time usage: 35.982s, average loss: [0.21014507 0.2635056 ]/0.236825\n",
      "-------------------------\n",
      "Epoch 145, time usage: 35.916s, average loss: [0.21027529 0.2635438 ]/0.236910\n",
      "-------------------------\n",
      "Epoch 146, time usage: 36.554s, average loss: [0.21021286 0.2637231 ]/0.236968\n",
      "-------------------------\n",
      "Epoch 147, time usage: 36.784s, average loss: [0.21026501 0.26361588]/0.236940\n",
      "-------------------------\n",
      "Epoch 148, time usage: 36.708s, average loss: [0.21028091 0.26364148]/0.236961\n",
      "-------------------------\n",
      "Epoch 149, time usage: 36.566s, average loss: [0.21069689 0.26380262]/0.237250\n",
      "-------------------------\n",
      "Epoch 150, time usage: 36.464s, average loss: [0.21012758 0.2643612 ]/0.237244\n",
      "-------------------------\n",
      "Epoch 151, time usage: 36.250s, average loss: [0.21010393 0.26342767]/0.236766\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00151.pt\n",
      "-------------------------\n",
      "Epoch 152, time usage: 36.285s, average loss: [0.21032457 0.2635113 ]/0.236918\n",
      "-------------------------\n",
      "Epoch 153, time usage: 36.360s, average loss: [0.2099276  0.26391226]/0.236920\n",
      "-------------------------\n",
      "Epoch 154, time usage: 36.356s, average loss: [0.21011667 0.2640977 ]/0.237107\n",
      "-------------------------\n",
      "Epoch 155, time usage: 36.183s, average loss: [0.21028192 0.2640322 ]/0.237157\n",
      "-------------------------\n",
      "Epoch 156, time usage: 36.565s, average loss: [0.2102991  0.26380068]/0.237050\n",
      "-------------------------\n",
      "Epoch 157, time usage: 36.638s, average loss: [0.21014139 0.26384082]/0.236991\n",
      "-------------------------\n",
      "Epoch 158, time usage: 36.285s, average loss: [0.21032467 0.2637705 ]/0.237048\n",
      "-------------------------\n",
      "Epoch 159, time usage: 36.272s, average loss: [0.21012335 0.26395476]/0.237039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: dial tcp 35.226.229.132:3307: connect: connection refused (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: dial tcp 35.226.229.132:3307: connect: connection refused (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: dial tcp 35.226.229.132:3307: connect: connection refused (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Epoch 160, time usage: 36.592s, average loss: [0.21059233 0.2634786 ]/0.237035\n",
      "-------------------------\n",
      "Epoch 161, time usage: 36.404s, average loss: [0.21053185 0.26384413]/0.237188\n",
      "-------------------------\n",
      "Epoch 162, time usage: 35.975s, average loss: [0.21026632 0.26430938]/0.237288\n",
      "-------------------------\n",
      "Epoch 163, time usage: 36.287s, average loss: [0.21010609 0.26366135]/0.236884\n",
      "-------------------------\n",
      "Epoch 164, time usage: 36.554s, average loss: [0.21026708 0.26387474]/0.237071\n",
      "-------------------------\n",
      "Epoch 165, time usage: 36.444s, average loss: [0.21038197 0.26400703]/0.237195\n",
      "-------------------------\n",
      "Epoch 166, time usage: 36.431s, average loss: [0.21062009 0.26354933]/0.237085\n",
      "-------------------------\n",
      "Epoch 167, time usage: 36.808s, average loss: [0.21053933 0.26409736]/0.237318\n",
      "-------------------------\n",
      "Epoch 168, time usage: 36.611s, average loss: [0.21000358 0.26347762]/0.236741\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00168.pt\n",
      "-------------------------\n",
      "Epoch 169, time usage: 36.254s, average loss: [0.21041615 0.2638662 ]/0.237141\n",
      "-------------------------\n",
      "Epoch 170, time usage: 36.470s, average loss: [0.21026345 0.26360297]/0.236933\n",
      "-------------------------\n",
      "Epoch 171, time usage: 36.257s, average loss: [0.21011233 0.26372492]/0.236919\n",
      "-------------------------\n",
      "Epoch 172, time usage: 36.216s, average loss: [0.21025087 0.2637323 ]/0.236992\n",
      "-------------------------\n",
      "Epoch 173, time usage: 36.432s, average loss: [0.21039651 0.26421598]/0.237306\n",
      "-------------------------\n",
      "Epoch 174, time usage: 36.485s, average loss: [0.2103614  0.26357082]/0.236966\n",
      "-------------------------\n",
      "Epoch 175, time usage: 36.421s, average loss: [0.21011496 0.26342654]/0.236771\n",
      "-------------------------\n",
      "Epoch 176, time usage: 36.509s, average loss: [0.21002536 0.26381618]/0.236921\n",
      "-------------------------\n",
      "Epoch 177, time usage: 36.305s, average loss: [0.21021676 0.26387316]/0.237045\n",
      "-------------------------\n",
      "Epoch 178, time usage: 36.024s, average loss: [0.21044588 0.2633514 ]/0.236899\n",
      "-------------------------\n",
      "Epoch 179, time usage: 35.933s, average loss: [0.21015957 0.26364383]/0.236902\n",
      "-------------------------\n",
      "Epoch 180, time usage: 36.431s, average loss: [0.210078   0.26394767]/0.237013\n",
      "-------------------------\n",
      "Epoch 181, time usage: 36.149s, average loss: [0.21012937 0.2634361 ]/0.236783\n",
      "-------------------------\n",
      "Epoch 182, time usage: 36.191s, average loss: [0.21017063 0.26346922]/0.236820\n",
      "-------------------------\n",
      "Epoch 183, time usage: 36.283s, average loss: [0.210273  0.2633028]/0.236788\n",
      "-------------------------\n",
      "Epoch 184, time usage: 36.194s, average loss: [0.20988601 0.26391542]/0.236901\n",
      "-------------------------\n",
      "Epoch 185, time usage: 36.392s, average loss: [0.21040979 0.26397562]/0.237193\n",
      "-------------------------\n",
      "Epoch 186, time usage: 36.294s, average loss: [0.21042341 0.26445866]/0.237441\n",
      "-------------------------\n",
      "Epoch 187, time usage: 36.172s, average loss: [0.21001372 0.26346847]/0.236741\n",
      "-------------------------\n",
      "Epoch 188, time usage: 36.155s, average loss: [0.21031009 0.26357988]/0.236945\n",
      "-------------------------\n",
      "Epoch 189, time usage: 36.059s, average loss: [0.21020117 0.26401016]/0.237106\n",
      "-------------------------\n",
      "Epoch 190, time usage: 36.180s, average loss: [0.21011777 0.26349944]/0.236809\n",
      "-------------------------\n",
      "Epoch 191, time usage: 36.162s, average loss: [0.20997106 0.2638691 ]/0.236920\n",
      "-------------------------\n",
      "Epoch 192, time usage: 36.741s, average loss: [0.21013795 0.26391825]/0.237028\n",
      "-------------------------\n",
      "Epoch 193, time usage: 36.483s, average loss: [0.21007836 0.26350626]/0.236792\n",
      "-------------------------\n",
      "Epoch 194, time usage: 36.568s, average loss: [0.20993839 0.26345888]/0.236699\n",
      "Model saved to  checkpoints/Sup_lr_0.0001_FC_128/model-00194.pt\n",
      "-------------------------\n",
      "Epoch 195, time usage: 36.674s, average loss: [0.21055628 0.26373816]/0.237147\n",
      "-------------------------\n",
      "Epoch 196, time usage: 36.659s, average loss: [0.21029161 0.26374444]/0.237018\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m feat, targ \u001b[38;5;241m=\u001b[39m feat\u001b[38;5;241m.\u001b[39mto(device), targ\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 47\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mvel_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m train_outs\u001b[38;5;241m.\u001b[39mappend(pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     49\u001b[0m train_targets\u001b[38;5;241m.\u001b[39mappend(targ\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cs236/project/cnn_vae_model.py:384\u001b[0m, in \u001b[0;36mVelocityRegressor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 384\u001b[0m     x, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvelocity_layers(x)\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cs236/project/cnn_vae_model.py:176\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    175\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_block(x)\n\u001b[0;32m--> 176\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    179\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape((shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_channel_size \u001b[38;5;241m*\u001b[39m DATA_LENGTH_AT_BOTTLENECK))\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cs236/project/cnn_vae_model.py:44\u001b[0m, in \u001b[0;36mBasicBlock1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_shortcut:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[1;32m     47\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs236/venvs/project/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vel_fc_dims = [128]\n",
    "dropout = 0.5\n",
    "lr = 1e-4\n",
    "vel_model = cnn_vae_model.VelocityRegressor(vae_model.enc, vel_fc_dims, num_outputs=2, dropout=dropout).to(device)\n",
    "\n",
    "def get_model_name():\n",
    "    name = \"Sup_lr_{}_FC\".format(lr)\n",
    "    for fc_dim in vel_fc_dims:\n",
    "        name += \"_{}\".format(fc_dim)\n",
    "    name += \"\" if freeze_encoder else \"_unfreeze_enc\"\n",
    "    return name\n",
    "\n",
    "# WANDB setup\n",
    "project_name = \"Pretrained-Encoder-With-Velocity-Decoder-RONIN-dataset-supervised\" + (\"\" if freeze_encoder else \"-unfreeze-encoder\")\n",
    "wandb_run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=project_name,\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"vel_fc_dims\": vel_fc_dims,\n",
    "        \"dropout\": dropout,\n",
    "        \"lr\": lr,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"encoder_first_chan_size\": first_chan_size,\n",
    "        \"encoder_last_chan_size\": last_chan_size,\n",
    "        \"encoder_fc_dim\": vae_fc_dim,\n",
    "        \"batch_size\": batch_size\n",
    "})\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(vel_model.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True, eps=1e-12)\n",
    "\n",
    "if 'start_epoch' not in locals():\n",
    "    start_epoch = 0\n",
    "\n",
    "max_epochs = 5000\n",
    "best_loss = np.inf\n",
    "train_losses_all = []\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "    start_t = time.time()\n",
    "    vel_model.train()\n",
    "    train_outs, train_targets = [], []\n",
    "    for batch_id, (feat, targ, _, _) in enumerate(train_loader):\n",
    "        feat, targ = feat.to(device), targ.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = vel_model(feat)\n",
    "        train_outs.append(pred.cpu().detach().numpy())\n",
    "        train_targets.append(targ.cpu().detach().numpy())\n",
    "        loss = criterion(pred, targ)\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_outs = np.concatenate(train_outs, axis=0)\n",
    "    train_targets = np.concatenate(train_targets, axis=0)\n",
    "    train_losses = np.average((train_outs - train_targets) ** 2, axis=0)\n",
    "\n",
    "    end_t = time.time()\n",
    "    print('-------------------------')\n",
    "    print('Epoch {}, time usage: {:.3f}s, average loss: {}/{:.6f}'.format(\n",
    "        epoch, end_t - start_t, train_losses, np.average(train_losses)))\n",
    "    avg_loss = np.average(train_losses)\n",
    "    train_losses_all.append(avg_loss)\n",
    "\n",
    "    wandb_run.log({\"vel_x_loss\": train_losses[0],\n",
    "                   \"vel_y_loss\": train_losses[1],\n",
    "                   \"avg_loss\": avg_loss})\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        utils.save_states(get_model_name(), epoch, vel_model, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
